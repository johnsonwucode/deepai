# Recurrent Neural Networks and Language Model

## Language Models


>P(w1,..., WT)  
>Word orderng: p(the cat is small) > p(small the is cat)  
> word choice: p(walking home after school)


## Tranditonal Language Model
* probability is usually conditioned on window of n previous words
* an incorrect but necessary Markov aussuption

  > P(w1..., wm) =
* to estimate probabilities, compute for unigrams and bigrams (conditioning one one/tow previous word(s)):
  >(p(w2)|p(w1)) = count(w1,w2)/count(w1)

>... Here she comes to wreck the day. $\int -xe^{x^2} dx$ it's because i'm
green isn't it! hey, maybe i will give you a call sometime. your number
still 911? kinda hot in $\int -xe^{x^2} dx$ these rhinos. look at that,
it's exactly three seconds before i honk your $\int -xe^{x^2} dx$ nose and
pull your underwear over your head ...
s
... Here she comes to wreck the day. it's because i'm green isn't it! hey,
maybe i will give you a call sometime. your number still 911?

> $$P(w_1,...,w_n) =
$$
R_{\mu \nu} - {1 \over 2}g_{\mu \nu}\,R + g_{\mu \nu} \Lambda
= {8 \pi G \over c^4} T_{\mu \nu}
$$

kinda hot in these rhinos. look at that, it's exactly three seconds before i
honk your nose and pull your underwear over your head ...


[**math reference**](http://csrgxtu.github.io/2015/03/20/Writing-Mathematic-Fomulars-in-Markdown/)
